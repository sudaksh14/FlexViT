{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9f6632",
   "metadata": {},
   "source": [
    "### Level switching\n",
    "\n",
    "Flex modules behave in much the same way as regular modules, but with the added functionality to change the level. So for a regular Conv2d, you might do this to create and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8694a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# creates a conv2d which takes 3 channels as input and gives 20 channels output\n",
    "conv = nn.Conv2d(3, 20, kernel_size=3)\n",
    "# creates a batch of 10 images with 3 channels of 100 by 100 pixels\n",
    "x = torch.rand((10, 3, 100, 100))\n",
    "y = conv(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c417f38",
   "metadata": {},
   "source": [
    "To make and run a flexible conv2d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flex_modules as fm\n",
    "\n",
    "# This creates a flexible conv2d with 3 levels, where the\n",
    "# 1st, 2nd, and 3rd take 1, 2, and 3 channels of input and\n",
    "# give 10, 15, and 20 channels output.\n",
    "conv = fm.Conv2d([1, 2, 3], [10, 15, 20], kernel_size=3)\n",
    "\n",
    "# set the conv to the maximum level, which takes 3 channels\n",
    "conv.set_level_use(conv.max_level())\n",
    "x = torch.rand((10, 3, 100, 100))\n",
    "y = conv(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can set it to a lower level, which only takes 2 channels of input, but\n",
    "# if we try to pass the same input of 3 channels to it, it will error.\n",
    "\n",
    "conv.set_level_use(1)\n",
    "try:\n",
    "    y = conv(x)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# This is why the first layer of any flexible model should always\n",
    "# have the same input dimensions, but output dimensions can differ.\n",
    "first_conv = fm.Conv2d([3, 3, 3], [10, 15, 20], kernel_size=3)\n",
    "\n",
    "for i in range(0, first_conv.max_level() + 1):\n",
    "    first_conv.set_level_use(i)\n",
    "    y = first_conv(x)\n",
    "    print(first_conv.current_level())\n",
    "    print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f1e85",
   "metadata": {},
   "source": [
    "This same system is also applicable to flexible models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2963759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import flexvit\n",
    "import utils\n",
    "\n",
    "# make flexible visual transformer with default config of 2 levels\n",
    "model = flexvit.ViTConfig().make_model().to(utils.get_device())\n",
    "x = torch.rand(10, 3, 224, 224).to(utils.get_device())\n",
    "\n",
    "for i in range(0, model.max_level() + 1):\n",
    "    model.set_level_use(i)\n",
    "    y = model(x)\n",
    "    print(model.current_level())\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c140c0",
   "metadata": {},
   "source": [
    "### Copying\n",
    "\n",
    "Being able to copy between regular layers and flexible layers can be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d371e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_nets(*nets: nn.Module):\n",
    "    for net in nets:\n",
    "        for p in net.parameters():\n",
    "            p.data[:] = torch.rand(*p.shape)\n",
    "\n",
    "reg = nn.Conv2d(10, 20, kernel_size=3)\n",
    "flex = fm.Conv2d([5, 7, 10], [10, 15, 20], kernel_size=3)\n",
    "x = torch.rand(10, 10, 20, 20)\n",
    "\n",
    "flex.set_level_use(2)\n",
    "\n",
    "# copying from regular to flexible\n",
    "randomize_nets(flex, reg)\n",
    "flex.load_from_base(reg)\n",
    "assert(torch.isclose(flex(x), reg(x)).all())\n",
    "\n",
    "# copying from flexible to regular\n",
    "randomize_nets(flex, reg)\n",
    "flex.copy_to_base(reg)\n",
    "assert(torch.isclose(flex(x), reg(x)).all())\n",
    "\n",
    "# creating a new base copy\n",
    "randomize_nets(flex, reg)\n",
    "reg2 = flex.make_base_copy()\n",
    "assert(torch.isclose(flex(x), reg2(x)).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458f93a",
   "metadata": {},
   "source": [
    "To copy entire networks is also possible with utils.flexible_model_copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import vit\n",
    "\n",
    "reg = vit.ViTConfig().make_model().to(utils.get_device())\n",
    "flex = flexvit.ViTConfig().make_model().to(utils.get_device())\n",
    "x = torch.rand(10, 3, 224, 224).to(utils.get_device())\n",
    "\n",
    "# copying from regular to flexible\n",
    "randomize_nets(flex, reg)\n",
    "utils.flexible_model_copy(reg, flex)\n",
    "assert(torch.isclose(flex(x), reg(x)).all())\n",
    "\n",
    "# copying from flexible to regular\n",
    "randomize_nets(reg, flex)\n",
    "utils.flexible_model_copy(flex, reg)\n",
    "assert(torch.isclose(flex(x), reg(x)).all())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
